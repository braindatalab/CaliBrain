{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This tutorial is available for download as a Jupyter notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Run benchmark experiments\n\nThis example demonstrates how to run comprehensive benchmarking experiments\nfor evaluating source localization algorithms using CaliBrain.\n\nThe script shows how to:\n- Set up ERP simulation parameters and logging\n- Configure parameter grids for MEG and EEG experiments  \n- Compare multiple algorithms (Gamma-MAP, eLORETA)\n- Evaluate uncertainty estimation and calibration performance\n- Generate comprehensive metrics and save results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport datetime\nimport logging\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nfrom calibrain import Benchmark, LeadfieldBuilder, MetricEvaluator, UncertaintyEstimator, SourceSimulator, SensorSimulator, gamma_map, eloreta\nfrom calibrain.utils import get_data_path\n\n# https://github.com/mne-tools/mne-python/blob/main/mne/_fiff/constants.py\n# print(fwd['info']['chs'][0]['unit'])  # Will show 107 (FIFF_UNIT_V)\n\ndef main():\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    os.makedirs(\"results/benchmark_results\", exist_ok=True)\n    os.makedirs(\"results/logs\", exist_ok=True)\n    log_file = f\"results/logs/benchmark_log_{timestamp}.log\"\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n        handlers=[\n            logging.FileHandler(log_file, mode=\"w\"),  # Overwrite log file each run\n            logging.StreamHandler()                   # Also print to console\n        ]\n    )\n    logger = logging.getLogger(__name__)\n\n    # n_trials = 4\n    ERP_config = {\n        \"tmin\": -0.5,\n        \"tmax\": 0.5,\n        \"stim_onset\": 0,\n        \"sfreq\": 250,\n        \"fmin\": 1,\n        \"fmax\": 5,\n        \"amplitude\": 50.0,\n        \"random_erp_timing\": True,\n        \"erp_min_length\": None,\n    }\n    \n    source_simulator = SourceSimulator(\n        ERP_config=ERP_config,\n        logger=logger\n    )\n\n    leadfield_dir = get_data_path()\n    leadfield_builder = LeadfieldBuilder(\n        leadfield_dir=leadfield_dir,\n        logger=logger,\n    )\n    \n    sensor_simulator = SensorSimulator(\n        logger=logger,\n    )\n\n    confidence_levels = np.arange(0.0, 1.1, 0.1) # 11 levels: [0.0, 0.1, ..., 1.0]\n    \n    uncertainty_estimator = UncertaintyEstimator(\n        confidence_levels=confidence_levels,\n        logger=logger,\n    )  \n      \n    # Define parameter grids for different data types\n    data_param_grid_meg = {\n        \"subject\": [\"CC120166\", \"CC120264\", \"CC120309\", \"CC120313\"],\n        \"nnz\": [1, 10, 50, 100],\n        \"orientation_type\": [\"fixed\"], # \"fixed\", \"free\"\n        \"alpha_SNR\": [0.0, 0.2, 0.4, 0.6, 0.8, 0.99],\n    }\n    \n    data_param_grid_eeg = {\n        \"subject\": [\"fsaverage\"], # \"caliBrain_fsaverage\", \"fsaverage\",\n        \"nnz\": [1, 10, 50, 100],\n        \"orientation_type\": [\"fixed\"], # \"fixed\", \"free\"\n        \"alpha_SNR\": [0.0, 0.2, 0.4, 0.6, 0.8, 0.99],\n    }\n        \n    gamma_map_params = {\n        \"init_gamma\": [0.001], #  0.001, 1.0, or tuple for random values (0.001, 0.1)   \n        \"noise_type\": [\"oracle\"], # \"baseline\", \"oracle\", \"joint_learning\", \"CV\"\n    }\n    \n    eloreta_params = {\n        \"noise_type\": [\"oracle\"],\n    }\n    \n    estimators = [\n        (gamma_map, gamma_map_params, data_param_grid_meg),\n        (eloreta, eloreta_params, data_param_grid_meg),\n        (gamma_map, gamma_map_params, data_param_grid_eeg),\n        (eloreta, eloreta_params, data_param_grid_eeg),\n    ]\n\n    metrics = [\n        \"mean_posterior_std\",               # Uncertainty\n        \"mean_calibration_error\",           # Calibration (auc)\n        \"max_underconfidence_deviation\",    # Calibration\n        \"max_overconfidence_deviation\",     # Calibration\n        \"mean_absolute_deviation\",          # Calibration\n        \"mean_signed_deviation\",            # Calibration\n        \"emd\",                              # spatial accuracy\n        \"jaccard_error\",                    # spatial accuracy\n        \"mse\",                              # spatial accuracy\n        \"euclidean_distance\",               # detection performance\n        \"f1\",                               # detection performance\n        \"accuracy\",                         # detection performance\n    ]\n\n    metric_evaluator = MetricEvaluator(\n        confidence_levels=confidence_levels,\n        metrics=metrics,\n        logger=logger\n    )\n\n    nruns = 1\n    # df = []\n    # for solver, solver_param_grid, data_param_grid in estimators:\n    #     benchmark = Benchmark(\n    #         solver=solver,\n    #         solver_param_grid=solver_param_grid,\n    #         data_param_grid=data_param_grid,\n    #         ERP_config=ERP_config,\n    #         source_simulator=source_simulator,\n    #         leadfield_builder=leadfield_builder,\n    #         sensor_simulator=sensor_simulator,\n    #         uncertainty_estimator=uncertainty_estimator,\n    #         metric_evaluator=metric_evaluator,\n    #         random_state=42,\n    #         logger=logger\n    #     )\n    #     results_df = benchmark.run(nruns=nruns)\n    #     df.append(results_df)\n\n    # results_df = pd.concat(df)\n    # results_df.to_csv(f\"results/benchmark_results/benchmark_results_{timestamp}.csv\", index=False)\n    \n    # print(results_df.head())\n\nif __name__ == \"__main__\":\n    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}