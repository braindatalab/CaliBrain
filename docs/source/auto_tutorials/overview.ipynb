{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This tutorial is available for download as a Jupyter notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Overview of brain source localization with CaliBrain\n\nThis tutorial covers the basic CaliBrain pipeline for brain source localization\nand uncertainty quantification: building forward models, simulating brain activity,\nestimating sources, and quantifying uncertainty. It introduces the core CaliBrain\ndata structures and components, covering the essential workflow at a high level.\nSubsequent tutorials address each topic in greater detail.\n\nCaliBrain is designed around a modular architecture where each component handles\na specific aspect of the source localization and uncertainty quantification pipeline.\n\nWe begin by importing the necessary Python modules:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Mohammad Orabe  <m.orabe@icloud.com>\n# License: AGPL-3.0 license\n# Copyright the CaliBrain contributors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The CaliBrain architecture\n\nCaliBrain follows a modular design with 8 main components that work together\nto provide a complete pipeline for brain source localization with uncertainty\nquantification:\n\n1. :class:`~calibrain.LeadfieldBuilder` - Creates the forward model\n2. :class:`~calibrain.SourceSimulator` - Generates brain source activity  \n3. :class:`~calibrain.SensorSimulator` - Simulates sensor measurements\n4. :class:`~calibrain.SourceEstimator` - Solves the inverse problem\n5. :class:`~calibrain.UncertaintyEstimator` - Quantifies estimate uncertainty\n6. :class:`~calibrain.MetricEvaluator` - Evaluates performance and calibration\n7. :class:`~calibrain.Visualizer` - Creates plots and visualizations\n8. :class:`~calibrain.Benchmark` - Orchestrates complete experiments\n\nThese components can be used individually for specific tasks or together \nthrough the :class:`~calibrain.Benchmark` class for automated workflows.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from calibrain import (\n    LeadfieldBuilder,\n    SourceSimulator,\n    SensorSimulator,\n    SourceEstimator,\n    UncertaintyEstimator,\n    MetricEvaluator,\n    Visualizer,\n    Benchmark\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the forward model\n\nThe :class:`~calibrain.LeadfieldBuilder` creates the forward model that maps\nbrain sources to sensor measurements. This is the foundation of all source\nlocalization analyses, as it defines the relationship between neural activity\nand what we observe at the sensors.\n\nThe forward model is represented by the leadfield matrix **L**, where:\n**sensor_data = L \u00d7 source_activity + noise**\n\nCaliBrain integrates with MNE-Python to provide realistic head models and\nsupports both EEG and MEG modalities with various source orientations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Configure the forward model\nleadfield_config = {\n    \"subject\": \"fsaverage\",           # Use MNE's template brain\n    \"spacing\": \"ico4\",                # Source space resolution  \n    \"modality\": \"eeg\",                # EEG sensors\n    \"orientation\": \"fixed\",           # Fixed source orientations\n    \"montage\": \"standard_1020\",       # Standard EEG layout\n}\n\n# # Build the leadfield matrix\n# leadfield_builder = LeadfieldBuilder(config=leadfield_config)\n# leadfield = leadfield_builder.simulate()\n\n# print(f\"Leadfield matrix shape: {leadfield.shape}\")\n# print(f\"  Sensors: {leadfield.shape[0]}\")  \n# print(f\"  Sources: {leadfield.shape[1]}\")\n\n# # %%\n# # The leadfield matrix dimensions tell us about our measurement setup:\n# # the number of sensors (EEG electrodes or MEG sensors) and the number\n# # of potential source locations in the brain. The condition number of\n# # this matrix affects the difficulty of the inverse problem.\n\n# condition_number = np.linalg.cond(leadfield)\n# print(f\"Condition number: {condition_number:.2e}\")\n\n# # %%\n# # Simulating brain source activity\n# # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# #\n# # The :class:`~calibrain.SourceSimulator` generates realistic brain source\n# # activity patterns with event-related potential (ERP) waveforms. This allows\n# # us to test source localization algorithms with known ground truth.\n\n# # Define the ERP characteristics\n# ERP_config = {\n#     \"tmin\": -0.2,          # Pre-stimulus period (seconds)\n#     \"tmax\": 0.5,           # Post-stimulus period (seconds)\n#     \"sfreq\": 250,          # Sampling frequency (Hz)\n#     \"amplitude\": 50.0,     # Peak amplitude (nAm)\n# }\n\n# # Initialize the source simulator\n# source_sim = SourceSimulator(ERP_config=ERP_config)\n\n# # Simulation parameters\n# n_sources = leadfield.shape[1]  # Use all available source locations\n# n_active = 3                    # Number of simultaneously active sources\n# n_trials = 20                   # Number of trials to simulate\n\n# # Generate source activity\n# source_data, active_indices = source_sim.simulate(\n#     n_sources=n_sources,\n#     n_active=n_active, \n#     n_trials=n_trials\n# )\n\n# print(f\"Source simulation completed:\")\n# print(f\"  Active sources: {len(active_indices)} out of {n_sources}\")\n# print(f\"  Data shape: {source_data.shape} (sources \u00d7 time \u00d7 trials)\")\n# print(f\"  Active indices: {active_indices}\")\n\n# # %%\n# # Let's visualize the simulated ERP waveforms to understand what we've created.\n# # We'll plot the time courses of the active sources and show the distribution\n# # of source amplitudes across the brain.\n\n# time = np.linspace(ERP_config['tmin'], ERP_config['tmax'], source_data.shape[1])\n\n# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# # Plot ERP time courses\n# for i, idx in enumerate(active_indices):\n#     ax1.plot(time, source_data[idx, :, 0], label=f'Source {idx}')\n# ax1.axvline(0, color='r', linestyle='--', alpha=0.7, label='Stimulus onset')\n# ax1.set_xlabel('Time (s)')\n# ax1.set_ylabel('Amplitude (nAm)')\n# ax1.set_title('Simulated ERP Waveforms')\n# ax1.legend()\n# ax1.grid(True, alpha=0.3)\n\n# # Show source amplitude distribution\n# all_amplitudes = np.max(np.abs(source_data), axis=(1, 2))\n# ax2.stem(range(len(all_amplitudes)), all_amplitudes, basefmt=' ')\n# ax2.set_xlabel('Source Index')\n# ax2.set_ylabel('Peak Amplitude (nAm)')\n# ax2.set_title('Source Amplitude Distribution')\n# ax2.set_yscale('log')\n# ax2.grid(True, alpha=0.3)\n\n# plt.tight_layout()\n# plt.show()\n\n# # %%\n# # Forward modeling: simulating sensor measurements\n# # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# #\n# # The :class:`~calibrain.SensorSimulator` applies the forward model to convert\n# # our simulated brain activity into realistic sensor measurements. It also adds\n# # configurable levels of measurement noise to simulate realistic recording\n# # conditions.\n\n# # Initialize sensor simulator\n# sensor_sim = SensorSimulator()\n\n# # Apply forward model with noise\n# alpha_SNR = 0.5  # Noise level (0 = no noise, 1 = pure noise)\n# sensor_clean, sensor_noisy = sensor_sim.simulate(\n#     source_data=source_data,\n#     leadfield=leadfield,\n#     alpha_SNR=alpha_SNR\n# )\n\n# # Calculate the actual signal-to-noise ratio\n# signal_power = np.var(sensor_clean)\n# noise_power = np.var(sensor_noisy - sensor_clean)\n# snr_db = 10 * np.log10(signal_power / noise_power) if noise_power > 0 else np.inf\n\n# print(f\"Sensor data generated:\")\n# print(f\"  Shape: {sensor_clean.shape} (channels \u00d7 time \u00d7 trials)\")\n# print(f\"  Signal-to-noise ratio: {snr_db:.1f} dB\")\n\n# # %%\n# # Solving the inverse problem\n# # ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# #\n# # The :class:`~calibrain.SourceEstimator` solves the inverse problem to estimate\n# # brain source activity from the sensor measurements. CaliBrain implements\n# # several source estimation methods with different characteristics.\n\n# # Initialize source estimator\n# source_est = SourceEstimator()\n\n# # Test different estimation methods\n# methods_to_test = ['eloreta', 'gamma_map']\n# source_estimates = {}\n\n# for method in methods_to_test:\n#     print(f\"\\nTesting {method.upper()} method...\")\n    \n#     try:\n#         # Estimate sources using the first trial\n#         estimated = source_est.estimate(\n#             sensor_data=sensor_noisy[:, :, 0],\n#             leadfield=leadfield,\n#             method=method,\n#             alpha=0.01  # Regularization parameter\n#         )\n        \n#         source_estimates[method] = estimated\n        \n#         # Calculate some basic metrics\n#         peak_amplitude = np.max(np.abs(estimated))\n#         active_threshold = peak_amplitude * 0.1\n#         n_detected = np.sum(np.max(np.abs(estimated), axis=1) > active_threshold)\n        \n#         print(f\"  Peak amplitude: {peak_amplitude:.2e}\")\n#         print(f\"  Sources detected: {n_detected}\")\n        \n#     except Exception as e:\n#         print(f\"  Error: {e}\")\n#         source_estimates[method] = None\n\n# # %%\n# # Let's visualize the source estimation results to compare the different methods\n# # and see how well they recover the true source locations.\n\n# if any(est is not None for est in source_estimates.values()):\n#     fig, axes = plt.subplots(2, len(methods_to_test), figsize=(12, 8))\n#     if len(methods_to_test) == 1:\n#         axes = axes.reshape(-1, 1)\n    \n#     for i, method in enumerate(methods_to_test):\n#         if source_estimates[method] is not None:\n#             estimated = source_estimates[method]\n            \n#             # Time course comparison\n#             ax = axes[0, i]\n#             for idx in active_indices[:2]:  # Show first 2 active sources\n#                 ax.plot(time, estimated[idx, :], '-', label=f'Est. {idx}')\n#                 ax.plot(time, source_data[idx, :, 0], '--', alpha=0.7, label=f'True {idx}')\n#             ax.axvline(0, color='gray', linestyle='--', alpha=0.5)\n#             ax.set_xlabel('Time (s)')\n#             ax.set_ylabel('Amplitude')\n#             ax.set_title(f'{method.upper()} - Time Courses')\n#             ax.legend()\n#             ax.grid(True, alpha=0.3)\n            \n#             # Source detection visualization\n#             ax = axes[1, i]\n#             est_amplitudes = np.max(np.abs(estimated), axis=1)\n#             colors = ['red' if idx in active_indices else 'blue' \n#                      for idx in range(len(est_amplitudes))]\n            \n#             ax.scatter(range(len(est_amplitudes)), est_amplitudes, \n#                       c=colors, alpha=0.6, s=20)\n#             ax.set_xlabel('Source Index')\n#             ax.set_ylabel('Peak Amplitude')\n#             ax.set_title(f'{method.upper()} - Detection (Red=True Active)')\n#             ax.set_yscale('log')\n#             ax.grid(True, alpha=0.3)\n#         else:\n#             for j in range(2):\n#                 axes[j, i].text(0.5, 0.5, f'{method.upper()}\\nNot Available', \n#                                ha='center', va='center', transform=axes[j, i].transAxes)\n    \n#     plt.tight_layout()\n#     plt.show()\n\n# # %%\n# # Quantifying uncertainty\n# # ^^^^^^^^^^^^^^^^^^^^^^^\n# #\n# # The :class:`~calibrain.UncertaintyEstimator` computes confidence intervals\n# # for the source estimates, allowing us to assess the reliability of our\n# # localization results. This is crucial for understanding which parts of\n# # our source estimates we can trust.\n\n# # Select the best available method for uncertainty quantification\n# best_method = 'eloreta' if source_estimates.get('eloreta') is not None else 'gamma_map'\n\n# if source_estimates[best_method] is not None:\n#     print(f\"Computing uncertainty for {best_method.upper()} estimates...\")\n    \n#     # Initialize uncertainty estimator\n#     uncertainty_est = UncertaintyEstimator()\n    \n#     # Compute 95% confidence intervals\n#     confidence_level = 0.95\n    \n#     try:\n#         lower_bounds, upper_bounds, point_estimates = uncertainty_est.estimate(\n#             sensor_data=sensor_noisy[:, :, :5],  # Use first 5 trials\n#             leadfield=leadfield,\n#             method=best_method,\n#             confidence_level=confidence_level,\n#             n_bootstrap=50  # Number of bootstrap samples\n#         )\n        \n#         print(f\"\u2713 Computed {confidence_level:.0%} confidence intervals\")\n        \n#         # Calculate coverage for the active sources\n#         true_signal = source_data[:, :, :5]  # First 5 trials to match\n#         within_bounds = ((true_signal >= lower_bounds[:, :, np.newaxis]) & \n#                         (true_signal <= upper_bounds[:, :, np.newaxis]))\n#         empirical_coverage = np.mean(within_bounds)\n        \n#         print(f\"Empirical coverage: {empirical_coverage:.1%} (target: {confidence_level:.0%})\")\n        \n#     except Exception as e:\n#         print(f\"Error in uncertainty estimation: {e}\")\n#         lower_bounds = upper_bounds = point_estimates = None\n\n# # %%\n# # Let's visualize the uncertainty estimates for a few of the active sources\n# # to see how well our confidence intervals capture the true variability.\n\n# if 'lower_bounds' in locals() and lower_bounds is not None:\n#     fig, axes = plt.subplots(1, min(2, len(active_indices)), figsize=(12, 4))\n#     if len(active_indices) == 1:\n#         axes = [axes]\n    \n#     for i, source_idx in enumerate(active_indices[:2]):\n#         ax = axes[i]\n        \n#         # Plot confidence intervals\n#         ax.fill_between(time, \n#                        lower_bounds[source_idx, :], \n#                        upper_bounds[source_idx, :],\n#                        alpha=0.3, color='blue', \n#                        label=f'{confidence_level:.0%} CI')\n        \n#         # Plot point estimate\n#         ax.plot(time, point_estimates[source_idx, :], \n#                 'b-', linewidth=2, label='Point estimate')\n        \n#         # Plot true signal from first trial\n#         ax.plot(time, source_data[source_idx, :, 0], \n#                 'r--', linewidth=2, label='True signal')\n        \n#         ax.axvline(0, color='gray', linestyle='--', alpha=0.7)\n#         ax.set_xlabel('Time (s)')\n#         ax.set_ylabel('Amplitude (nAm)')\n#         ax.set_title(f'Source {source_idx} Uncertainty')\n#         ax.legend()\n#         ax.grid(True, alpha=0.3)\n    \n#     plt.tight_layout()\n#     plt.show()\n\n# # %%\n# # Performance evaluation\n# # ^^^^^^^^^^^^^^^^^^^^^^\n# #\n# # The :class:`~calibrain.MetricEvaluator` assesses the quality of our source\n# # estimates and uncertainty quantification. This is essential for validating\n# # our methods and comparing different approaches.\n\n# if source_estimates[best_method] is not None:\n#     print(f\"Evaluating {best_method.upper()} performance...\")\n    \n#     # Initialize metric evaluator\n#     metric_eval = MetricEvaluator()\n    \n#     try:\n#         # Compute localization metrics\n#         localization_metrics = metric_eval.compute_localization_metrics(\n#             true_sources=source_data[:, :, 0],  # Use first trial as reference\n#             estimated_sources=source_estimates[best_method],\n#             true_active_indices=active_indices\n#         )\n        \n#         print(\"\\n\ud83d\udcca Localization Performance:\")\n#         for metric, value in localization_metrics.items():\n#             print(f\"  {metric}: {value:.4f}\")\n        \n#         # Compute calibration metrics if uncertainty is available\n#         if 'lower_bounds' in locals() and lower_bounds is not None:\n#             calibration_metrics = metric_eval.compute_calibration_metrics(\n#                 true_sources=source_data[:, :, :5],  # Match uncertainty data\n#                 lower_bounds=lower_bounds,\n#                 upper_bounds=upper_bounds,\n#                 confidence_level=confidence_level\n#             )\n            \n#             print(f\"\\n\ud83d\udcc8 Calibration Quality ({confidence_level:.0%} CI):\")\n#             for metric, value in calibration_metrics.items():\n#                 print(f\"  {metric}: {value:.4f}\")\n        \n#     except Exception as e:\n#         print(f\"Error in performance evaluation: {e}\")\n\n# # %%\n# # Automated benchmarking workflows\n# # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# #\n# # The :class:`~calibrain.Benchmark` class orchestrates complete experimental\n# # workflows for systematic method comparison and parameter optimization. This\n# # is particularly useful for large-scale studies or when you need to test\n# # multiple conditions automatically.\n\n# print(\"\\n\ud83d\udd2c Benchmark Workflow Overview\")\n# print(\"=\" * 40)\n\n# # Define a parameter grid for systematic evaluation\n# param_grid = {\n#     \"subject\": [\"fsaverage\"],\n#     \"nnz\": [1, 3, 5],                    # Number of active sources\n#     \"orientation_type\": [\"fixed\"],        # Source orientations  \n#     \"alpha_SNR\": [0.3, 0.5, 0.7],       # Noise levels\n# }\n\n# print(\"Example parameter combinations:\")\n# total_combinations = len(param_grid[\"nnz\"]) * len(param_grid[\"alpha_SNR\"])\n# for nnz in param_grid[\"nnz\"]:\n#     for alpha in param_grid[\"alpha_SNR\"]:\n#         print(f\"  \u2022 {nnz} active sources, \u03b1_SNR = {alpha}\")\n\n# print(f\"\\nTotal combinations: {total_combinations}\")\n\n# # Initialize benchmark (demonstration only)\n# benchmark = Benchmark(\n#     ERP_config=ERP_config,\n#     data_param_grid=param_grid,\n#     experiment_dir=\"./benchmark_results\"\n# )\n\n# print(\"To run the complete benchmark:\")\n# print(\"results = benchmark.run(nruns=10)\")\n# print(\"This would generate a comprehensive performance database.\")\n\n# # %%\n# # Creating comprehensive visualizations\n# # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# #\n# # The :class:`~calibrain.Visualizer` creates publication-ready figures for\n# # all aspects of the analysis. It can automatically generate comprehensive\n# # reports including time series, topographies, brain maps, and calibration plots.\n\n# # Initialize visualizer\n# viz = Visualizer(base_save_path=\"./tutorial_figures\")\n\n# print(\"\ud83d\udcca Visualization capabilities:\")\n# print(\"  \u2022 ERP time series and source activity plots\")\n# print(\"  \u2022 Sensor topographies and brain source maps\") \n# print(\"  \u2022 Uncertainty quantification visualizations\")\n# print(\"  \u2022 Calibration analysis and performance curves\")\n# print(\"  \u2022 Automated report generation\")\n\n# # Example of creating a summary figure (conceptual)\n# print(\"\\nTo create comprehensive visualizations:\")\n# print(\"viz.create_analysis_summary(source_data, sensor_data, estimates, uncertainty)\")\n\n# # %%\n# # .. _calibrain-workflow-summary:\n# #\n# # Summary and next steps\n# # ^^^^^^^^^^^^^^^^^^^^^^\n# #\n# # This tutorial covered the essential CaliBrain workflow for brain source\n# # localization with uncertainty quantification:\n# #\n# # \u2705 **Forward modeling**: Built leadfield matrix with :class:`~calibrain.LeadfieldBuilder`\n# #\n# # \u2705 **Source simulation**: Generated realistic ERPs with :class:`~calibrain.SourceSimulator`\n# #\n# # \u2705 **Sensor simulation**: Applied forward model and noise with :class:`~calibrain.SensorSimulator`\n# #\n# # \u2705 **Source estimation**: Solved inverse problem with :class:`~calibrain.SourceEstimator`\n# #\n# # \u2705 **Uncertainty quantification**: Computed confidence intervals with :class:`~calibrain.UncertaintyEstimator`\n# #\n# # \u2705 **Performance evaluation**: Assessed quality with :class:`~calibrain.MetricEvaluator`\n# #\n# # \u2705 **Automated workflows**: Demonstrated systematic evaluation with :class:`~calibrain.Benchmark`\n# #\n# # The modular design allows you to use individual components for specific tasks\n# # or combine them for complete analyses. The uncertainty quantification capabilities\n# # make CaliBrain particularly suitable for rigorous assessment of source localization\n# # reliability.\n\n# print(\"\\n\ud83c\udfaf Next steps:\")\n# print(\"  \u2022 Explore the detailed component tutorials\")\n# print(\"  \u2022 Try the examples with your own data\")\n# print(\"  \u2022 Read the API documentation for parameter details\")\n# print(\"  \u2022 Join the community discussions on GitHub\")\n# print(\"\\n\ud83e\udde0 Happy source localizing!\")\n\n# ##############################################################################\n# # The subsequent tutorials dive deeper into each component, covering advanced\n# # features like custom source estimation methods, sophisticated uncertainty\n# # analysis, large-scale benchmarking workflows, and integration with real\n# # EEG/MEG data. CaliBrain's focus on uncertainty quantification makes it\n# # particularly valuable for robust neuroscience research."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}